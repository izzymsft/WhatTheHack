{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge 02 - OpenAI Models & Capabilities"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this challenge, you will learn about the different capabilities of OpenAI models and learn how to choose the best model for your use case.\r\n",
        "\r\n",
        "Questions you will be able to answer by the end of this challenge:\r\n",
        "\r\n",
        "* How do responses differ for each model?\r\n",
        "* What are ways to benchmark the performance of models? "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Overview on finding the right model for you\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Model Families\r\n",
        "\r\n",
        "Azure OpenAI provides access to many different models, grouped by family and capability. A model family typically associates models by their intended task. \r\n",
        "\r\n",
        "Model families currently available in Azure OpenAI includes GPT-4, GPT-3, Codex and Embeddings. Please reference this link for more inofrmation: https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models\r\n",
        "\r\n",
        "\r\n",
        "*Some models are not available for new deployments beginning **July 6, 2023**. Deployments created prior to July 6, 2023 remain available to customers until **July 5, 2024**. You may revise the environment file and the model you deploy accordingly. Please refer to the following link for more details: https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/legacy-models"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Model Capacities\r\n",
        "The GPT-3 models can understand and generate natural language. The service offers four model capabilities, each with different levels of power and speed suitable for different tasks. Davinci is the most capable model, while Ada is the fastest. The following list represents the latest versions of GPT-3 models, ordered by increasing capability.\r\n",
        "\r\n",
        "- text-ada-001\r\n",
        "- text-babbage-001\r\n",
        "- text-curie-001\r\n",
        "- text-davinci-003"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\r\n",
        "[Azure OpenAI models](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models)  \r\n",
        "\r\n",
        "\r\n",
        "| | Similarity embedding | Text search embedding | Code Search Embedding |\r\n",
        "| --- | --- | --- |\r\n",
        "| |These models are good at capturing **semantic similarity** between two or more pieces of text. | These models help measure whether long documents are relevant to a short search query. There are two input types supported by this family: **doc**, for embedding the documents to be retrieved, and **query**, for embedding the search query. |Similar to text search embedding models, there are two input types supported by this family: **code**, for embedding code snippets to be retrieved, and text, for embedding natural language search queries. | \r\n",
        "|**Use cases** | Clustering, regression, anomaly detection, visualization | Search, context relevance, information retrieval | Code search and relevance |\r\n",
        "|**Models** |text-similarity-ada-001 <br> text-similarity-babbage-001 <br> text-similarity-curie-001 <br> text-similarity-davinci-001 | text-search-ada-doc-001  <br> text-search-ada-query-001  <br> text-search-babbage-doc-001  <br> text-search-babbage-query-001  <br> text-search-curie-doc-001  <br>text-search-curie-query-001  <br> text-search-davinci-doc-001  <br> text-search-davinci-query-001 | code-search-ada-code-001 <br> code-search-ada-text-001 <br> code-search-babbage-code-001 <br> code-search-babbage-text-001 | \r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Model Taxonomy  \r\n",
        "Let's choose a general text GPT-3 model, using the second most powerful model (Curie)\r\n",
        "\r\n",
        "**Model taxonomy**: {capability} - {family} - {input-type} - {identifier}  \r\n",
        "\r\n",
        "{family}     --> text   (general text GPT-3 model)  \r\n",
        "{capacity} --> curie  (curie is second most powerful in ada-babbage-curie-davinci family)  \r\n",
        "{input-type} --> n/a    (only specified for search models)  \r\n",
        "{identifier} --> 001    (version 001)  \r\n",
        "\r\n",
        "model = \"text-curie-001\""
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Element | Description |\r\n",
        "| --- | --- |\r\n",
        "|**{family}** | The model family of the model. For example, GPT-3 models uses text, while Codex models use code.|\r\n",
        "|**{capacity}** | The relative capacity of the model. For example, GPT-3 models include ada, babbage, curie, and davinci, with increasing capacity. |\r\n",
        "|**{input-type}** | (Embeddings models only) The input type of the embedding supported by the model. For example, text search embedding models support doc and query. | \r\n",
        "|**{identifier}** |The version identifier of the model.| \t\r\n",
        "\r\n",
        "\t\r\n",
        "\r\n",
        "\r\n",
        "\t\r\n",
        "\t"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While Davinci is the most capable, the other models provide significant speed advantages. Our recommendation is for users to start with Davinci while experimenting, because it produces the best results and validate the value that Azure OpenAI can provide. Once you have a prototype working, you can then optimize your model choice with the best latency/performance balance for your application."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Pricing Details\r\n",
        "\r\n",
        "For the most up-to-date information, check out the Azure OpenAI [pricing page](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/).\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Quotas and Limits\r\n",
        "\r\n",
        "*The below limits are subject to change. We anticipate that you will need higher limits as you move toward production and your solution scales. When you know your solution requirements, please reach out to us by applying for a quota increase here: https://aka.ms/oai/quotaincrease\r\n",
        "\r\n",
        "|Limit Name\t|Limit Value|\r\n",
        "|---|---|\r\n",
        "|OpenAI resources per region per Azure subscription|\t3|\r\n",
        "| Requests per minute per model* | Davinci-models (002 and later): 120<br>ChatGPT model: 300<br>GPT-4 models: 18<br>All other models: 300                                             |\r\n",
        "| Tokens per minute per model*   | Davinci-models (002 and later): 40,000<br>ChatGPT model: 120,000<br>GPT-4 8k model: 10,000<br>GPT-4 32k model: 32,000<br>All other models: 120,000 |\r\n",
        "|Max fine-tuned model deployments*\t|2|\r\n",
        "|Ability to deploy same model to multiple deployments\t|Not allowed|\r\n",
        "|Total number of training jobs per resource|\t100|\r\n",
        "|Max simultaneous running training jobs per resource|\t1|\r\n",
        "|Max training jobs queued\t|20|\r\n",
        "|Max Files per resource\t|50|\r\n",
        "|Total size of all files per resource\t|1 GB|\r\n",
        "|Max training job time (job will fail if exceeded)\t|720 hours|\r\n",
        "|Max training job size (tokens in training file) x (# of epochs)\t|2 Billion|"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6 Model Best Use Cases\r\n",
        "\r\n",
        "Here is some general guidance on well-suited applications that tend to differentiate models. Note that these are not hard and fast rules, and oftentimes experimentation and benchmarking are important to making the best decision for your solution.\r\n",
        "\r\n",
        "|Model|Use Cases|\r\n",
        "|---|---|\r\n",
        "|Davinci| Complex intent, cause and effect, summarization for audience|\r\n",
        "|Curie|Language translation, complex classification, text sentiment, summarization|\r\n",
        "|Babbage|Moderate classification, semantic search classification|\r\n",
        "|Ada|Parsing text, simple classification, address correction, keywords|"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7 Model Selection Best Practices\r\n",
        "While Davinci is the most capable, the other models can provide significant advantages such as speed (low latency). Our recommendation is for users to start with Davinci while experimenting, because it produces the best results and validate the value that Azure OpenAI can provide. \r\n",
        "\r\n",
        "Once you have a prototype working, you can then optimize your model choice with the best latency/performance balance for your application."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Let's Start Implementation"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you don't already have the OpenAI, Python-dotenv, plotly, or scikit-learn packages installed on your compute, the following cells will install them."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade pandas\r\n",
        "%pip install --upgrade openai\r\n",
        "%pip install --upgrade python-dotenv\r\n",
        "%pip install --upgrade plotly scikit-learn"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1685909662455
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\r\n",
        "import os\r\n",
        "import json\r\n",
        "from dotenv import load_dotenv, find_dotenv\r\n",
        "load_dotenv(find_dotenv())\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686331269276
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up your environment to access your OpenAI keys. Refer to your OpenAI resource in the Azure Portal to retrieve information regarding your OpenAI endpoint and keys.\r\n",
        "\r\n",
        "For security purposes, store your sensitive information in a .env file."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = os.getenv(\"OPENAI_API_KEY\")\r\n",
        "assert API_KEY, \"ERROR: Azure OpenAI Key is missing\"\r\n",
        "openai.api_key = API_KEY\r\n",
        "\r\n",
        "RESOURCE_ENDPOINT = os.getenv(\"OPENAI_API_BASE\",\"\").strip()\r\n",
        "assert RESOURCE_ENDPOINT, \"ERROR: Azure OpenAI Endpoint is missing\"\r\n",
        "assert \"openai.azure.com\" in RESOURCE_ENDPOINT.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\r\n",
        "\r\n",
        "openai.api_base = RESOURCE_ENDPOINT\r\n",
        "openai.api_type = \"azure\"\r\n",
        "openai.api_version = \"2023-03-15-preview\"\r\n",
        "\r\n",
        "chat_model=os.getenv(\"CHAT_MODEL_NAME\")\r\n",
        "davinci_model=os.getenv(\"TEXT_DAVINCI_MODEL_NAME\")\r\n",
        "curie_model=os.getenv(\"TEXT_CURIE_MODEL_NAME\")\r\n",
        "babbage_model=os.getenv(\"TEXT_BABBAGE_MODEL_NAME\")\r\n",
        "ada_model=os.getenv(\"TEXT_ADA_MODEL_NAME\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686331271142
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.0 Helper Functions\r\n",
        "Throughout this course, we will use OpenAI's `gpt-3.5-turbo` model and the [chat completions endpoint](https://platform.openai.com/docs/guides/chat). \r\n",
        "\r\n",
        "This helper function will make it easier to use prompts and look at the generated outputs."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**timer wrapper** helps us monitor and compare the latency of each model.\r\n",
        "\r\n",
        "**get_completion** helps create openAI response using text completion model of your choice.\r\n",
        "\r\n",
        "**get_chat_completion** helps create openAI response using chat model of your choice.\r\n",
        "\r\n",
        "**get_completion_from_messages** helps create openAI response using chat model of your choice, enabling chat history.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\r\n",
        "import time\r\n",
        "\r\n",
        "def timer(func):\r\n",
        "    @functools.wraps(func)\r\n",
        "    def wrapper(*args, **kwargs):\r\n",
        "        start_time = time.perf_counter()\r\n",
        "        value = func(*args, **kwargs)\r\n",
        "        end_time = time.perf_counter()\r\n",
        "        run_time = end_time - start_time\r\n",
        "        print(\"Finished {} in {} secs\".format(repr(func.__name__), round(run_time, 3)))\r\n",
        "        return value[0], value[1], round(run_time, 3)\r\n",
        "\r\n",
        "    return wrapper"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686334202300
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timer\r\n",
        "def get_completion(prompt, model=davinci_model):\r\n",
        "    response = openai.Completion.create(\r\n",
        "        engine=model,\r\n",
        "        prompt=prompt,\r\n",
        "        temperature=0, # this is the degree of randomness of the model's output\r\n",
        "        max_tokens = 500,\r\n",
        "        top_p = 1.0,\r\n",
        "    )\r\n",
        "    return response.choices[0].text, response['usage']['total_tokens']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686334262004
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timer\r\n",
        "def get_chat_completion(prompt, model=chat_model):\r\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\r\n",
        "    response = openai.ChatCompletion.create(\r\n",
        "        model=model,\r\n",
        "        messages=messages,\r\n",
        "        temperature=0, # this is the degree of randomness of the model's output\r\n",
        "        max_tokens = 200,\r\n",
        "        top_p = 1.0,\r\n",
        "    )\r\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686334263077
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timer\r\n",
        "def get_completion_from_messages(messages, model=chat_model, temperature=0):\r\n",
        "    response = openai.ChatCompletion.create(\r\n",
        "        engine=model,\r\n",
        "        messages=messages,\r\n",
        "        temperature=temperature, # this is the degree of randomness of the model's output\r\n",
        "    )\r\n",
        "    #print(str(response.choices[0].message))\r\n",
        "    return response.choices[0].message[\"content\"]\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686334263773
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Summarize Text"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "model_pricing = pd.DataFrame(columns=['model', 'price', 'time'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686334268062
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = f\"\"\"\r\n",
        "The Olympic Games Tokyo 2020 reached a global broadcast audience of 3.05 billion people, according to independent research conducted on behalf of the International Olympic Committee (IOC). Official coverage on Olympic broadcast partners\\' digital platforms alone generated 28 billion video views in total – representing a 139 per cent increase compared with the Olympic Games Rio 2016 and underlining the changing media landscape and Tokyo 2020\\'s designation as the first streaming Games and the most watched Olympic Games ever on digital platforms.Sony and Panasonic partnered with NHK to develop broadcasting standards for 8K resolution television, with a goal to release 8K television sets in time for the 2020 Summer Olympics. In early 2019, Italian broadcaster RAI announced its intention to deploy 8K broadcasting for the Games. NHK broadcast the opening and closing ceremonies, and coverage of selected events in 8K. Telecom company NTT Docomo signed a deal with Finland\\'s Nokia to provide 5G-ready baseband networks in Japan in time for the Games.The Tokyo Olympics were broadcast in the United States by NBCUniversal networks, as part of a US$4.38 billion agreement that began at the 2014 Winter Olympics in Sochi. The United States Olympic & Paralympic Committee asserted that a \"right of abatement\" clause in the contract was triggered by the delay of the Games to 2021, requiring the IOC to \"negotiate in good faith an equitable reduction in the applicable broadcast rights payments\" by NBC, which remains one of IOC\\'s biggest revenue streams. According to NBCUniversal CEO Jeff Shell, the Tokyo games could be the most profitable Olympics in NBC\\'s history. The Tokyo games were NBC\\'s first Olympics broadcast under current president Susan Rosner Rovner.In Europe, this was the first Summer Olympics under the IOC\\'s exclusive pan-European rights deal with Eurosport, which began at the 2018 Winter Olympics and is contracted to run through 2024. The rights for the 2020 Summer Olympics covered almost all of Europe; a pre-existing deal with a marketer excludes Russia. Eurosport planned to sub-license coverage to free-to-air networks in each territory, and other channels owned by Discovery, Inc. subsidiaries. In the United Kingdom, these were set to be the last Games with rights owned primarily by the BBC, although as a condition of a sub-licensing agreement due to carry into the 2022 and 2024 Games, Eurosport holds exclusive pay television rights. In France, these were the last Games whose rights are primarily owned by France Télévisions. Eurosport debuted as pay television rightsholder, after Canal+ elected to sell its pay television rights as a cost-saving measure.In Canada, the 2020 Games were shown on CBC/Radio-Canada platforms, Sportsnet, TSN and TLN. In Australia, they were aired by Seven Network. In the Indian subcontinent, they were aired by Sony Pictures Networks India (SPN).\r\n",
        "\"\"\"\r\n",
        "prompt = f\"\"\"\r\n",
        "Summarize the text delimited by triple backticks into a single sentence.\r\n",
        "```{text}```\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "davinci_response, davinci_price, davinci_time = get_completion(prompt, model=davinci_model)\r\n",
        "curie_response, curie_price, curie_time = get_completion(prompt, model=curie_model)\r\n",
        "babbage_response, babbage_price, babbage_time = get_completion(prompt, model=babbage_model)\r\n",
        "ada_response, ada_price, ada_time = get_completion(prompt, model=ada_model)\r\n",
        "print(f\"Davinci Response: {davinci_response}\\n\")\r\n",
        "print(f\"Curie Response: {curie_response}\\n\")\r\n",
        "print(f\"Babbage Response: {babbage_response}\\n\")\r\n",
        "print(f\"Ada Response: {ada_response}\\n\")\r\n",
        "\r\n",
        "new_rows = pd.DataFrame([{'model': 'davinci', 'price': davinci_price, 'time': davinci_time},\r\n",
        "                                       {'model': 'curie', 'price': curie_price, 'time': curie_time},\r\n",
        "                                       {'model': 'babbage', 'price': babbage_price, 'time': babbage_time},\r\n",
        "                                       {'model': 'ada', 'price': ada_price, 'time': ada_time}])\r\n",
        "model_pricing = pd.concat([model_pricing, new_rows], ignore_index=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686334278657
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_**Takeaway: Davinci and Curie models are more suitable for tasks like summarization. The answer is more concise and takes less time.**_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Student Challenge #1:\r\n",
        "With tactics learned in CH1, edit the prompt to get more concise answer from assistant. Do you find any difference in the result?"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# edit the prompt to get more concise answer from assistant"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Summarization for a targeted audience"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\r\n",
        "Summarize the text delimited by triple backticks into a single sentence for 7-year-old to understand.\r\n",
        "```{text}```\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "davinci_response, davinci_price, davinci_time = get_completion(prompt, model=davinci_model)\r\n",
        "curie_response, curie_price, curie_time = get_completion(prompt, model=curie_model)\r\n",
        "babbage_response, babbage_price, babbage_time = get_completion(prompt, model=babbage_model)\r\n",
        "ada_response, ada_price, ada_time = get_completion(prompt, model=ada_model)\r\n",
        "print(f\"Davinci Response: {davinci_response}\\n\")\r\n",
        "print(f\"Curie Response: {curie_response}\\n\")\r\n",
        "print(f\"Babbage Response: {babbage_response}\\n\")\r\n",
        "print(f\"Ada Response: {ada_response}\\n\")\r\n",
        "\r\n",
        "new_rows = pd.DataFrame([{'model': 'davinci', 'price': davinci_price, 'time': davinci_time},\r\n",
        "                                       {'model': 'curie', 'price': curie_price, 'time': curie_time},\r\n",
        "                                       {'model': 'babbage', 'price': babbage_price, 'time': babbage_time},\r\n",
        "                                       {'model': 'ada', 'price': ada_price, 'time': ada_time}])\r\n",
        "model_pricing = pd.concat([model_pricing, new_rows], ignore_index=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686332538379
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Student Challenge #2:\r\n",
        "Edit the prompt to summarize the text for eye-catching newspaper title. Compare different results."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit the prompt to summarize the text for eye-catching newspaper title"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Summarize Cause & Effect"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\r\n",
        "Summarize the major event's cause and effect for the text delimited by triple backticks into a single sentence less than 50 words.\r\n",
        "```{text}```\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "davinci_response, davinci_price, davinci_time = get_completion(prompt, model=davinci_model)\r\n",
        "curie_response, curie_price, curie_time = get_completion(prompt, model=curie_model)\r\n",
        "babbage_response, babbage_price, babbage_time = get_completion(prompt, model=babbage_model)\r\n",
        "ada_response, ada_price, ada_time = get_completion(prompt, model=ada_model)\r\n",
        "print(f\"Davinci Response: {davinci_response}\\n\")\r\n",
        "print(f\"Curie Response: {curie_response}\\n\")\r\n",
        "print(f\"Babbage Response: {babbage_response}\\n\")\r\n",
        "print(f\"Ada Response: {ada_response}\\n\")\r\n",
        "\r\n",
        "new_rows = pd.DataFrame([{'model': 'davinci', 'price': davinci_price, 'time': davinci_time},\r\n",
        "                                       {'model': 'curie', 'price': curie_price, 'time': curie_time},\r\n",
        "                                       {'model': 'babbage', 'price': babbage_price, 'time': babbage_time},\r\n",
        "                                       {'model': 'ada', 'price': ada_price, 'time': ada_time}])\r\n",
        "model_pricing = pd.concat([model_pricing, new_rows], ignore_index=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686332587257
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Student Challenge #3: Model Comparison\r\n",
        "Use the model comparison chart to briefly summarize your findings after comparing different model output & time taken. eg. Davinci: Performance (+++), time (+). You may also leverage other python packages to visualize your findings.\r\n",
        "\r\n",
        "|Model| Performance  |Time|\r\n",
        "|---|---|---|\r\n",
        "|Davinci|||\r\n",
        "|Curie|||\r\n",
        "|Babbage|||\r\n",
        "|Ada|||\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### Student Challenge #4: Text Classification\r\n",
        " Edit the prompt to make the models generate key topic categories for the text. Compare different model performance."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit the prompt to make the models generate key topic categories for the text"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Student Challenge #5:\r\n",
        "Edit the prompt to make the models generate more precise results. Compare different model performance."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit the prompt to make the models generate more precise results. "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Student Challenge #6: Model Comparison\r\n",
        "\r\n",
        "Write code to create two bar charts comparing the **price** and **time for completion** between the models. We recommend using the `matplotlib.pyplot` library for making visualizations.\r\n",
        "\r\n",
        "Instructions for completion:\r\n",
        "\r\n",
        "* Utilize the `model_comparison` dataframe to calculate the averages of price and time for each model\r\n",
        "* Produce the bar chart in a currency amount. Note that the `price` column in the `model_comparison` dataframe is in the unit of tokens. Refer to the Azure [OpenAI pricing page](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/) to convert the units."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" STUDENT CHALLENGE \"\"\"\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "### 1. Bar chart to compare pricing\r\n",
        "\r\n",
        "\r\n",
        "### 2. Bar chart to compare time for completion"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Generate Nick Names"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Student Challenge #7:\r\n",
        "Use different models to create nick names for players from examples words. Compare different model performance. (You can set the temperature value high to increase randomness and more innovative responses.)\r\n",
        "\r\n",
        "Player description: The champion of Men's 100 metre freestyle swimming. Seed words: fast, strong, talented.Nick names: Swimming Genius, Dark Horse, 100-Metre-Freestyle Killer\r\n",
        "\r\n",
        "Player description: The champion of Women Figure Skating. Seed words: elegant, talented, soft."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code:"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1685916265011
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Comparison\r\n",
        "|Model| Performance  |Time|Tokens|Pricing |\r\n",
        "|---|---|---|\r\n",
        "|Davinci|||||\r\n",
        "|Curie|||||\r\n",
        "|Babbage|||||\r\n",
        "|Ada|||||"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Embeddings\r\n",
        "This section focuses on how to retrieve embeddings using different embedding models, and find similarity between documents. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Student Challenge #8:\r\n",
        "Compare the summaries of two swimming games at the 2020 Summer Olympics using the data provided below.\r\n",
        "\r\n",
        "See whether there are differences using different embedding models to compare."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai.embeddings_utils import get_embedding, cosine_similarity"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686117865502
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "game_summary = [\r\n",
        "    \"The mixed 100 metre medley relay event at the 2020 Summer Olympics was held in 2021 at the Tokyo Aquatics Centre. These Games marked the first time to feature a mixed-gender swimming event in the program. Each 4-person team features two male and two female swimmers in no particular order. The medals for the competition were presented by Kirsty Coventry IOC Executive Board Member, Zimbabwe; Olympian, 2 Gold Medals, 4 Silver Medals, 1 Bronze Medal, and the medalists bouquets were presented by Errol Clarke, FINA Bureau Member; Barbados.\",\r\n",
        "    \"The men's 200 metre breaststroke event at the 2020 Summer Olympics was held from 27 to 29 July 2021 at the Tokyo Aquatics Centre. It was the event's twenty-sixth consecutive appearance, having been held at every edition since 1908.\"\r\n",
        "]\r\n",
        "\r\n",
        "game_highlight = [\r\n",
        "    'The 2020 Summer Olympics featured the first ever mixed-gender swimming event, the 100 metre medley relay. Medals were presented by Kirsty Coventry and bouquets by Errol Clarke.',\r\n",
        "    \"The men's 200 metre breaststroke event was held at the 2020 Summer Olympics in Tokyo, making it the event's 26th consecutive appearance since 1908.\"\r\n",
        "]\r\n",
        "\r\n",
        "olympics_game_df = pd.DataFrame({\"summary\":game_summary, \"qualification\":game_highlight})\r\n",
        "\r\n",
        "olympics_game_df.head()   "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686117698204
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timer\r\n",
        "def get_embedding(text, model=\"text-embedding-ada\"):\r\n",
        "    response = openai.Embedding.create(\r\n",
        "        input=text,\r\n",
        "        engine=model\r\n",
        "    )\r\n",
        "    return response[\"data\"][0][\"embedding\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686117707487
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"text-embedding-ada\"\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686117710151
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"text-similarity-ada\"\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686117715107
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"text-similarity-curie\"\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686117718971
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Comparison\r\n",
        "|Model| Performance  |Time|\r\n",
        "|---|---|---|\r\n",
        "|text-embedding-ada-002|||\r\n",
        "|text-similarity-ada-001|||\r\n",
        "|text-similarity-curie-001|||"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}