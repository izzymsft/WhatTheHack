{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk you through the concepts of tokens and chunking. In the previous notebook, we were able to provide some additional context and ground the models. Is there a limit to the amount of additional context we can provide the model? Unfortunately, the answer is yes. A limit exists for the number of tokens that are allowed in the input and the output combined. \n",
    "\n",
    "What are tokens? Tokens are a representation of how the Azure OpenAI models process text. They are words or just chunks of characters. Let's look at the total number of tokens in the response we got back from the grounding notebook. There are many ways to calculate tokens. In this notebook, we will take a look at the tiktoken library to count the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed modules\n",
    "import openai\n",
    "import PyPDF3\n",
    "import os\n",
    "import json\n",
    "import tiktoken\n",
    "import spacy\n",
    "from openai.error import InvalidRequestError\n",
    "\n",
    "# python -m spacy download en_core_web_sm\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "from spacy.lang.en import English \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Load your OpenAI credentials\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert API_KEY, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "RESOURCE_ENDPOINT = os.getenv(\"OPENAI_API_BASE\",\"\").strip()\n",
    "assert RESOURCE_ENDPOINT, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in RESOURCE_ENDPOINT.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = RESOURCE_ENDPOINT\n",
    "\n",
    "openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "model=os.getenv(\"TEXT_DAVINCI_NAME\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiktoken uses a technique called BPE, or byte pair encoding to convert the given text into tokens. There are different encodings available to help process the words. In this notebook, we will use the cl100k_base.\n",
    "\n",
    "Let's count the number of tokens in the answer we received from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_tokens(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Kaiser Permanente offers four types of individual and family plans: Copay plans, Deductible plans, Virtual plans, and HSA qualified plans. Catastrophic plans are also available in some markets.\"\"\"\n",
    "\n",
    "count_tokens(text, \"cl100k_base\")\n",
    "\n",
    "print(\"There are \" + str(count_tokens(text, \"cl100k_base\")) + \" tokens in this sentence: \" + text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we want to add in more context than what we put in the text variable? Let's say we want to provide more context with a PDF file. Let's try to count the number of tokens again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = open(r'C:\\Users\\dthakar\\Documents\\WhatTheHack\\xxx-OpenAIFundamentals\\Student\\Resources\\data\\CH3-data.pdf', 'rb') \n",
    "doc_helper = PyPDF3.PdfFileReader(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaltext = ''\n",
    "totalpages = doc_helper.getNumPages()\n",
    "for eachpage in range(totalpages):\n",
    "   p = doc_helper.getPage(eachpage)\n",
    "   indpagetext = p.extractText()\n",
    "   finaltext += indpagetext\n",
    "\n",
    "clean_text = finaltext.replace(\"  \", \" \").replace(\"\\n\", \"; \").replace(';',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"What is the answer to the following question regarding the PDF document?\\n\\n{finaltext}\\n\\n\" \n",
    "q = \"Can you give me a summary of the document?\"\n",
    "\n",
    "try:\n",
    "    final_prompt = prompt + q\n",
    "    response = openai.Completion.create(engine=model, prompt=final_prompt, max_tokens=50)\n",
    "    answer = response.choices[0].text.strip()\n",
    "    print(f\"{q}\\n{answer}\\n\")\n",
    "\n",
    "except InvalidRequestError as e:\n",
    "    print(e.error)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will get an error message after running the above snippet of code. The model reaches its maximum context length. For GPT-3 models, the token limit is 4097 tokens. How do we fix this issue by giving it all of the needed context, but not running into the token limit issue?\n",
    "\n",
    "To solve this problem, we can take a look at a concept called Chunking. Chunking helps limit the amount of information we pass into the model. The information that we will pass through are the most relevant chunks from the overall data. There are many considerations that come into play when chunking. For example, you need to figure out the best chunk size. If the chunks are too small, you may lose important context. If the chunks are too big, it may contain unnecessary information. \n",
    "\n",
    "Below are some common chunking techniques.\n",
    "\n",
    "1. Chunking with smaller chunks \n",
    "2. Chunking by splitting sentences  \n",
    "3. Chunking with sentence overlap \n",
    "4. Chunking recursively \n",
    "\n",
    "Let us take a look at these techniques in action."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Chunking with smaller chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The sun was setting over the horizon, casting a warm glow over the landscape. Birds chirped in the trees, and a gentle breeze rustled the leaves. In the distance, a herd of deer grazed in a meadow. The air was filled with the sweet scent of blooming flowers. It was a peaceful and serene scene, perfect for a quiet evening stroll.\"\n",
    "chunks = text.split()\n",
    "\n",
    "for chunk in chunks: \n",
    "    print(chunk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In method one, we see an example of what not to do when chunking. The chunks are so small that the individual chunks make no sense without the rest of the context. This is an important key when chunking - you want to keep the semantic meaning. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: Chunking by splitting sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Today was a fun day. I had lots of ice cream. I also met my best friend Sally and we played together at the new playground.\"\n",
    "\n",
    "for sentence in nlp(text).sents:\n",
    "    print(sentence.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In method two, we see a better example of chunking. Here we are using the spaCy library to chunk the text into individual sentences. This can be useful when you are trying to do text summarization. You can rank the individual sentences and use the top results in the summary.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3:  Chunking with sentence overlap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The sun was setting over the horizon, casting a warm glow over the landscape. Birds chirped in the trees, and a gentle breeze rustled the leaves. In the distance, a herd of deer grazed in a meadow. The air was filled with the sweet scent of blooming flowers. It was a peaceful and serene scene, perfect for a quiet evening stroll.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "sentences = list(doc.sents)\n",
    "overlap = 1\n",
    "chunks =[]\n",
    "\n",
    "for i in range(len(sentences) - overlap):\n",
    "    chunk = sentences[i : i + overlap + 1]\n",
    "    chunks.append(chunk)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print([sent.text for sent in chunk])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In method three, we see an example of chunking with ensures the semantic meaning is kept. In other words, the context is preserved between the sentences. This is especially important when you are searching data for relevant results or when you are summarizing a piece of text. It is important to capture the relationships between the sentences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 4: Chunking recursively using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 30\n",
    ")\n",
    "docs = split_text.create_documents([clean_text])\n",
    "docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In method four, we see an example of chunking using langchain, a popular framework for creating applications using large language models. In the previous methods you saw various examples of chunking. Langchain can help make the chunking process easier with some of its methods. These methods include fixed size chunks as well as recursive chunking, which we saw above.\n",
    "\n",
    "For example, there is CharacterTextSplitter which will split the given text into a fixed size chunk of a given size and a given overlap of characters. \n",
    "\n",
    "RecursiveCharacterTextSplitter divides the text into smaller chunks in an iterative manner. Again, you can provide the chunk size and chunk overlap count. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, chunking is an important technique for many reasons. It helps bypass the token limit when working with lots of data and also optimizes the response we get back from the model. Finding the right chunking technique and chunk size is crucial to receiving relevant responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
