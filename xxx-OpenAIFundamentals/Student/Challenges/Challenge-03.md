# Challenge 03 - Grounding, Chunking, and Embedding

[< Previous Challenge](./Challenge-02.md) - **[Home](../README.md)** - [Next Challenge >](./Challenge-04.md)

***This is a template for a single challenge. The italicized text provides hints & examples of what should or should NOT go in each section.  You should remove all italicized & sample text and replace with your content.***

## Pre-requisites (Optional)

* Install the required libraries in the requirements.txt file via ```pip install -r requirements.txt ```

## Introduction

When working with large language models, it is important to understand how to ground them with the right data. In addition, you will take a look at how to deal with token limits when you have a lot of data. Finally, you will experiment with embeddings. This challenge will teach you all the fundamental concepts before you see them in play in Challenge 4.

## Description

Questions you should be able to answer by the end of the challenge:

*Why is grounding important and how can you ground a LLM model?

*What is a token limit?

*How can you deal with token limits? What are techniques of chunking?

*What do embedding help accomplish?

Sections in this Challenge:

1. 3.1 - Grounding
2. 3.2 - Chunking
3. 3.3 - Embeddings
   
## Success Criteria

To complete this challenge successfully, you should be able to:
- Identify the simplest method for grounding
- Understand chunking techniques
- Create embeddings 

## Additional Resources 

* [Grounding LLMs](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/grounding-llms/ba-p/3843857)
* [Embeddings example](https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb)
* [Langchain Chunking](https://js.langchain.com/docs/modules/indexes/text_splitters/examples/recursive_character)
  
