# What The Hack - Azure OpenAI Fundamentals

## Introduction

The Azure OpenAI Fundamentals Hack is an introduction to understanding the conceptual foundations of Azure OpenAI models. Materials from this Hack can serve as a foundation for building your own solution with Azure OpenAI.

This Hack consists of five challenges and is meant to be self-administered, so anyone can complete the material independently. Whether you have limited to no experience with Machine Learning or have experimented with OpenAI before but want a deeper understanding of how to implement an AI solution, this Hack is for you.

-- normally, team based activity ... but pull in a friend to work with..

## Learning Objectives

This hack is for anyone who wants to gain hands-on experience experimenting with prompt engineering and machine learning best practices, and apply them to generate effective responses from ChatGPT and OpenAI models.

Participants will learn how to:

- Compare OpenAI models and choose the best one for a scenario

- Use prompt engineering techniques on complex tasks

- Manage large amounts of data within token limits, including the use of chunking and chaining techniques

- Grounding models to avoid hallucinations or false information

- Implement embeddings using search retrieval techniques
Evaluate models for truthfulness and monitor for PII detection in model interactions
## Challenges

- Challenge 00: **[Prerequisites - Ready, Set, GO!](Student/Challenges/Challenge-00.md)**
	 - Prepare your workstation to work with Azure.
- Challenge 01: **[Prompt Engineering](Student/Challenges/Challenge-01.md)**
	 - What's posssible through Prompt Engineering 
	 - Best practices when using OpenAI text and chat models
- Challenge 02: **[OpenAI Models & Capabilities](Student/Challenges/Challenge-02.md)**
	 - What are the capacities of each Azure OpenAI model?
	 - How to select the right model for your application
- Challenge 03: **[Grounding, Chunking, and Embedding](Student/Challenges/Challenge-03.md)**
	 - Why is grounding important and how can you ground a Large Language Model (LLM)?
	 - What is a token limit? How can you deal with token limits? What are techniques of chunking?
- Challenge 04: **[Retrieval Augmented Generation (RAG)](Student/Challenges/Challenge-04.md)**
	 - How do we create ChatGPT-like experiences on Enterprise data? In other words, how do we "ground" powerful LLMs to primarily our own data?
- Challenge 05: **[Responsible AI](Student/Challenges/Challenge-05.md)**
	 - What are services and tools to identify and evaluate harms and data leakage in LLMs?
	 - What are ways to evaluate truthfulness and reduce hallucinations?
What are methods to evaluate a model if you don't have a ground truth dataset for comparison?

## Prerequisites

- Azure subscription
- Access to Azure OpenAI
- Jupyter Notebook editor (we recommend Visual Studio Code or Azure Machine Learning Studio)
- Python, Pip

## Contributors

- Amanda Wong
- Devanshi Thakar
- Ellie Nosrat
- Juan Llovet de Casso
- Melody Yin
- Rachel Liu
- Shiva Chittamuru
